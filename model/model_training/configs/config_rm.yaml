defaults_rm:
  rng_seed: 0xa1221f97
  is_reward_model: true
  pooling: last
  learning_rate: 1e-5
  gradient_checkpointing: false
  gradient_accumulation_steps: 2
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1e-12
  weight_decay: 0.00
  warmup_steps: 10
  eval_steps: 50
  save_steps: 100
  save_strategy: steps
  max_length: 512
  num_train_epochs: 2
  logging_steps: 10
  max_grad_norm: 2.0
  save_total_limit: 4
  dtype: fp16
  eval_accumulation_steps:
  freeze_layer:
  cache_dir: .cache
  loss_fn: RMLoss
  score_l2_reg: 0.001
  eval_size:
  log_dir: "base"
  quantization: false
  seq2seqmodel: false
  fuse_gelu: true
  log_wandb: true
  verbose: false
  output_dir: .saved_models_rm
  use_custom_sampler: false
  residual_dropout: 0.0
  use_flash_attention: false
  sort_by_length: false
  per_digit_tokens: false
  datasets_extra: []
  metrics: ["accuracy", "kendalltau"]
  deepspeed_config: configs/zero_config.json
  max_replies: 5
  residual_dropout_lima: false
  use_system_tag: false
  system_property_dropout: 0.5
  system_add_length: false

### new
gold-rm:
  rng_seed: 0xa1221f97
  is_reward_model: true
  pooling: last
  learning_rate: 1e-5
  gradient_checkpointing: false
  gradient_accumulation_steps: 2
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1e-12
  weight_decay: 0.00
  warmup_steps: 10
  eval_steps: 50
  save_steps: 100
  save_strategy: steps
  max_length: 512
  num_train_epochs: 2
  logging_steps: 10
  max_grad_norm: 2.0
  save_total_limit: 10
  dtype: fp16
  eval_accumulation_steps:
  freeze_layer:
  datasets:
    - anthropic_rlhf:
        max_val_set: 500
    - shp:
        max_val_set: 500
    - hellaswag:
        max_val_set: 500
    - webgpt:
        val_split: 0.2
        max_val_set: 500
    - hf_summary_pairs:
        max_val_set: 500
    - oasst_export:
        max_val_set: 500
  datasets_extra: [] # For config options to add additional datasets, since yaml doesn't let us extend arrays
  cache_dir: .cache
  loss_fn: RMLoss
  score_l2_reg: 0.001
  eval_size:
  log_dir: "base"
  quantization: false
  seq2seqmodel: false
  fuse_gelu: true
  log_wandb: true
  verbose: true
  use_custom_sampler: false
  residual_dropout: 0.0
  use_flash_attention: false
  sort_by_length: false
  per_digit_tokens: false
  datasets_extra: []
  metrics: ["accuracy", "kendalltau"]
  deepspeed_config: configs/zero_config.json
  max_replies: 5
  residual_dropout_lima: false
  use_system_tag: false
  system_property_dropout: 0.5
  system_add_length: false

### new
proxy-rm:
  rng_seed: 0xa1221f97
  is_reward_model: true
  pooling: last
  learning_rate: 1e-5
  gradient_checkpointing: false
  gradient_accumulation_steps: 2
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1e-12
  weight_decay: 0.00
  warmup_steps: 10
  eval_steps: 50
  save_steps: 100
  save_strategy: steps
  max_length: 512
  num_train_epochs: 2
  logging_steps: 10
  max_grad_norm: 2.0
  save_total_limit: 10
  dtype: fp16
  eval_accumulation_steps:
  freeze_layer:
  datasets:
    - synthetic_anthropic_rlhf:
        max_val_set: 500
    - synthetic_shp:
        max_val_set: 500
    - synthetic_hellaswag:
        max_val_set: 500
    - synthetic_webgpt:
        val_split: 0.2
        max_val_set: 500
    - synthetic_hf_summary_pairs:
        max_val_set: 500
    - synthetic_oasst_export:
        max_val_set: 500
  datasets_extra: [] # For config options to add additional datasets, since yaml doesn't let us extend arrays
  cache_dir: .cache
  loss_fn: RMLoss
  score_l2_reg: 0.001
  eval_size:
  log_dir: "base"
  quantization: false
  seq2seqmodel: false
  fuse_gelu: true
  log_wandb: true
  verbose: true
  use_custom_sampler: false
  residual_dropout: 0.0
  use_flash_attention: false
  sort_by_length: false
  per_digit_tokens: false
  datasets_extra: []
  metrics: ["accuracy", "kendalltau"]
  deepspeed_config: configs/zero_config.json
  max_replies: 5
  residual_dropout_lima: false
  use_system_tag: false
  system_property_dropout: 0.5
  system_add_length: false
  lhf_directory: /root/haikang/Open-Assistant-hfa/lhf


###############################################
# Model and Dataset Configs
###############################################


### new
llama2-7b-gold-rm:
  is_reward_model: true
  pooling: last
  datasets:
    - anthropic_rlhf:
        # fraction: 0.1
        max_val_set: 1000
    - shp:
        max_val_set: 1000
    - hellaswag:
        # fraction: 0.5
        max_val_set: 1000
    - webgpt:
        val_split: 0.2
        max_val_set: 1000
    - hf_summary_pairs:
        # fraction: 0.1
        max_val_set: 1000
    - oasst_export:
        # fraction: 0.1
        max_val_set: 1000
  sort_by_length: false
  use_custom_sampler: true
#   model_name: "meta-llama/Llama-2-7b-hf"
  model_name: /root/haikang/Open-Assistant-hfa/lhf/saved_models/gold-sft/llama2-7b        # path to reward model with sft weights
  output_dir: /root/haikang/Open-Assistant-hfa/lhf/saved_models/gold-rm/llama2-7b
  learning_rate: 5e-6
  residual_dropout: 0.0
  weight_decay: 0.0
  max_length: 4096
  use_flash_attention: true
  gradient_checkpointing: true
  warmup_steps: 1000
  dtype: bf16
  gradient_accumulation_steps: 1
  per_device_train_batch_size: 12
  per_device_eval_batch_size: 12
  num_train_epochs: 1
  eval_steps: 1000
  save_steps: 2000
  deepspeed_config: configs/zero3_config_sft.json
  use_system_tag: false
  system_property_dropout: 0.5
  system_add_length: false
  verbose: True
  patch_unsupported: True

test:
    datasets:
        - anthropic_rlhf:
            max_val_set: 500
    gradient_accumulation_steps: 10
    per_device_train_batch_size: 12
    per_device_eval_batch_size: 12
    eval_steps: 100
    save_steps: 1000
    model_name: /root/haikang/benchmarking-human-feedback-alignment/oa_out/sft/llama2-7b/checkpoint-2000
    output_dir: /root/haikang/benchmarking-human-feedback-alignment/oa_out/rm/llama2-7b-test
    learning_rate: 5e-6
    epochs: 1
    # per_device_train_batch_size: 32
    # per_device_eval_batch_size: 32


### new
llama2-7b-proxy-rm:
  is_reward_model: true
  pooling: last
  datasets:
    - synthetic_anthropic_rlhf:
        # fraction: 0.1
        max_val_set: 1000
    - synthetic_shp:
        max_val_set: 1000
    - synthetic_hellaswag:
        # fraction: 0.5
        max_val_set: 1000
    - synthetic_webgpt:
        val_split: 0.2
        max_val_set: 1000
    - synthetic_hf_summary_pairs:
        # fraction: 0.1
        max_val_set: 1000
    - synthetic_oasst_export:
        # fraction: 0.1
        max_val_set: 1000
  sort_by_length: false
  use_custom_sampler: true
#   model_name: "meta-llama/Llama-2-7b-hf"
  model_name: /root/haikang/Open-Assistant-hfa/lhf/saved_models/lm-sft/llama2-7b        # path to reward model with sft weights
  output_dir: /root/haikang/Open-Assistant-hfa/lhf/saved_models/proxy-rm/llama2-7b
  learning_rate: 5e-6
  residual_dropout: 0.0
  weight_decay: 0.0
  max_length: 4096
  use_flash_attention: true
  gradient_checkpointing: true
  warmup_steps: 1000
  dtype: bf16
  gradient_accumulation_steps: 1
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  num_train_epochs: 1
  eval_steps: 1000
  save_steps: 2000
  deepspeed_config: configs/zero3_config_sft.json
  use_system_tag: false
  system_property_dropout: 0.5
  system_add_length: false
  verbose: True
  patch_unsupported: True
  lhf_directory: /root/haikang/Open-Assistant-hfa/lhf


oasst-rm-1-pythia-6.9b:
  is_reward_model: true
  pooling: last
  datasets:
    - augment_oasst:
        input_file_path: augmented_latin_cyrillic_oasst_2023-03-27_v2.jsonl
    - anthropic_rlhf:
        fraction: 0.1
        max_val_set: 1000
    - shp:
        max_val_set: 1000
    - hellaswag:
        fraction: 0.5
        max_val_set: 1000
    - webgpt:
        val_split: 0.05
        max_val_set: 1000
    - hf_summary_pairs:
        fraction: 0.1
        max_val_set: 250
  sort_by_length: false
  use_custom_sampler: true
  model_name: andreaskoepf/pythia-6.9b-gpt4all-pretrain
  learning_rate: 1e-5
  residual_dropout: 0.0
  weight_decay: 0.0
  max_length: 2048
  use_flash_attention: true
  gradient_checkpointing: true
  warmup_steps: 50
  dtype: float16
  gradient_accumulation_steps: 4
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 4
  num_train_epochs: 3
  eval_steps: 251
  save_steps: 500
  deepspeed_config: configs/zero3_config_sft.json
  use_system_tag: false
  system_property_dropout: 0.5
  system_add_length: false

use_system_tag:
  use_system_tag: True
  system_property_dropout: 0.5
  system_add_length: True

oasst-rm-1-pythia-2.8b:
  is_reward_model: true
  pooling: last
  datasets:
    - oasst_export:
        lang: "en,es,de,fr"
        hf_dataset_name: OpenAssistant/oasst1
        val_split: 0.1
    - augment_oasst:
        input_file_path: augmented_latin_cyrillic_oasst_2023-03-27_v2.jsonl
    - anthropic_rlhf:
        fraction: 0.1
        max_val_set: 1000
    - shp:
        max_val_set: 1000
    - hellaswag:
        fraction: 0.5
        max_val_set: 1000
    - webgpt:
        val_split: 0.05
        max_val_set: 1000
    - hf_summary_pairs:
        fraction: 0.1
        max_val_set: 250
  use_custom_sampler: true
  sort_by_length: false
  model_name: andreaskoepf/pythia-2.8b-gpt4all-pretrain
  learning_rate: 1e-5
  residual_dropout: 0.01
  weight_decay: 0.0
  dtype: float32
  max_length: 2048
  use_flash_attention: true
  gradient_checkpointing: true
  warmup_steps: 50
  gradient_accumulation_steps: 4
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 5
  num_train_epochs: 3
  eval_steps: 251
  save_steps: 500
  deepspeed_config: configs/zero3_config_sft.json

oasst-rm-1-pythia-1.4b:
  is_reward_model: true
  pooling: last
  datasets:
    - oasst_export:
        lang: "en,es,de,fr"
        hf_dataset_name: OpenAssistant/oasst1
        val_split: 0.1
    # - augment_oasst:
    #     input_file_path: augmented_latin_cyrillic_oasst_2023-03-27.jsonl
    - anthropic_rlhf:
        fraction: 0.1
        max_val_set: 1000
    - shp:
        max_val_set: 1000
    - hellaswag:
        fraction: 0.5
        max_val_set: 1000
    - webgpt:
        val_split: 0.05
        max_val_set: 1000
    - hf_summary_pairs:
        fraction: 0.1
        max_val_set: 250
  use_custom_sampler: true
  sort_by_length: false
  model_name: andreaskoepf/pythia-1.4b-gpt4all-pretrain
  learning_rate: 8e-6
  residual_dropout: 0.01
  weight_decay: 0.0
  dtype: float32
  max_length: 2048
  use_flash_attention: true
  warmup_steps: 50
  gradient_accumulation_steps: 4
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 5
  num_train_epochs: 2
  eval_steps: 500
  save_steps: 1000
