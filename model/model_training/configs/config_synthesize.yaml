# mostly borrowed from config_rl.yaml
gold-rm-synthesize:
  rng_seed: 0xa1221f97
  is_reward_model: true
  pooling: last
  gradient_accumulation_steps: 2
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  dtype: fp16
  eval_accumulation_steps:
  freeze_layer:
  datasets:
    - anthropic_rlhf
    - shp
    - hellaswag
    - webgpt:
        val_split: 0.2
    - hf_summary_pairs
    - oasst_export:
        val_split: 0.2
  datasets_extra: [] # For config options to add additional datasets, since yaml doesn't let us extend arrays
  cache_dir: .cache
  loss_fn: RMLoss
  score_l2_reg: 0.001
  eval_size:
  log_dir: "base"
  quantization: false
  seq2seqmodel: false
  fuse_gelu: true
  log_wandb: true
  verbose: true
  use_custom_sampler: false
  residual_dropout: 0.0
  use_flash_attention: false
  sort_by_length: false
  per_digit_tokens: false
  datasets_extra: []
  metrics: ["accuracy", "kendalltau"]
  deepspeed_config: configs/zero_config.json
  max_replies: 5
  residual_dropout_lima: false
  use_system_tag: false
  system_property_dropout: 0.5
  system_add_length: false


###############################################
# Model and Dataset Configs
###############################################


### new
llama2-7b-gold-rm-synthesize:
  is_reward_model: true
  pooling: last
  datasets:
    - oasst_export:
        val_split: 0.2
    - webgpt:
        val_split: 0.2
    - shp
    - hellaswag
    - hf_summary_pairs
    - anthropic_rlhf
  sort_by_length: false
  model_name: /root/haikang/Open-Assistant-hfa/lhf/saved_models/gold-rm/llama2-7b
  max_length: 2048
  use_flash_attention: true
  gradient_checkpointing: true
  dtype: bf16
  per_device_train_batch_size: 12
  per_device_eval_batch_size: 12
  deepspeed_config: configs/zero3_config_sft.json
  max_replies: 5
  use_system_tag: false
  system_property_dropout: 0.5
  system_add_length: false
  verbose: True
  patch_unsupported: True
  lhf_directory: /root/haikang/Open-Assistant-hfa/lhf

synthesize-test:
  per_device_train_batch_size: 12
  per_device_eval_batch_size: 12
  datasets:
    - oasst_export:
        val_split: 0.2
    - webgpt:
        val_split: 0.2


# ListDataset: 6574 (1.846391%)
# Subset of WebGPT (webgpt): 16923 (4.753038%)
# SHPDataset (SHP): 38993 (10.951675%)
# HellaSwagDataset (hellaswag): 39905 (11.207821%)
# HFSummaryPairs: 92858 (26.080338%)
# AnthropicRLHF (anthropic_rlhf): 160793 (45.160738%)