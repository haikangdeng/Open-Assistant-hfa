import argparse
import logging
import os
from typing import Callable, Literal, Optional, Sequence, Union, Dict, List, Any
from dataclasses import dataclass

import datasets
import torch
from transformers import LlamaTokenizer, LlamaTokenizerFast
from model_training.custom_datasets.ranking_collator import RankingDataCollator
from model_training.custom_datasets.formatting import DatasetEntryRm, format_pairs, format_reply
from model_training.efficiency_utils import fuse_gelu
from model_training.metrics import RewardMetrics
from model_training.utils.utils import (
    PerDatasetSampler,
    _strtobool,
    get_dataset,
    get_dataset_rm,
    get_loss,
    get_model,
    get_tokenizer,
    init_rng,
    read_yamls,
)
from torch import nn
from torch.utils.data import DataLoader, Subset
from tqdm import tqdm
from transformers import PreTrainedModel, Trainer, TrainingArguments, PreTrainedTokenizerBase
from transformers.trainer_pt_utils import IterableDatasetShard
from transformers.trainer_utils import seed_worker
from transformers.training_args import OptimizerNames
from transformers.utils import is_datasets_available
from trl.trainer.utils import DPODataCollatorWithPadding
from trl.trainer import DPOTrainer
from torch.nn.utils.rnn import pad_sequence


@dataclass
class CustomDPODataCollatorWithPadding(DPODataCollatorWithPadding):
    """
    https://github.com/huggingface/trl/blob/main/trl/trainer/utils.py#L265
    Rewrite the DPODataCollatorWithPadding class to
        1. suit for this project's dataset format
        2. get rid of additional BOS token generated by Llama tokenizer
    """
    tokenizer: PreTrainedTokenizerBase
    model: Optional[PreTrainedModel] = None
    padding: Union[bool, str] = True
    max_length: Optional[int] = None
    min_prefix_length: int = 256
    label_pad_token_id: int = -100
    padding_value: int = 0
    truncation_mode: str = "keep_end"
    is_encoder_decoder: Optional[bool] = False
    # max_target_length: Optional[int] = None
    max_replies: Optional[int] = 5
    use_system_tag: bool = False
    system_property_dropout: float = 0.5
    system_add_length: bool = True

    def tokenize_batch_element(
        self,
        prompt: str,
        chosen: str,
        rejected: str,
    ) -> Dict:
        """Tokenize a single batch element.

        At this stage, we don't convert to PyTorch tensors yet; we just handle the truncation
            in case the prompt + chosen or prompt + rejected responses is/are too long. First
            we truncate the prompt; if we're still too long, we truncate the chosen/rejected.

        We also create the labels for the chosen/rejected responses, which are of length equal to
            the sum of the length of the prompt and the chosen/rejected response, with
            label_pad_token_id  for the prompt tokens.
        """
        batch = {}

        if not self.is_encoder_decoder:
            chosen_tokens = self.tokenizer(chosen, add_special_tokens=False)
            rejected_tokens = self.tokenizer(rejected, add_special_tokens=False)
            prompt_tokens = self.tokenizer(prompt, add_special_tokens=False)
            
            # for llama2 tokenization, remove additional <s> at the beginning of a reply
            if isinstance(self.tokenizer, (LlamaTokenizer, LlamaTokenizerFast)):
                chosen_tokens["input_ids"] = chosen_tokens["input_ids"][1:]
                chosen_tokens["attention_mask"] = chosen_tokens["attention_mask"][1:]
                rejected_tokens["input_ids"] = rejected_tokens["input_ids"][1:]
                rejected_tokens["attention_mask"] = rejected_tokens["attention_mask"][1:]

            eos_token_id = self.tokenizer.eos_token_id
            # Get indices in list prompt_tokens["input_ids"] that equals the EOS token (often 0)
            eos_indices_prompt = [i for i, x in enumerate(prompt_tokens["input_ids"]) if x == eos_token_id]
            # attention mask these indices to eos_token_id
            new_attention_mask = [
                0 if i in eos_indices_prompt else p for i, p in enumerate(prompt_tokens["attention_mask"])
            ]
            prompt_tokens["attention_mask"] = new_attention_mask

            # do the same for chosen and rejected
            eos_indices_chosen = [i for i, x in enumerate(chosen_tokens["input_ids"]) if x == eos_token_id]
            new_attention_mask_c = [
                0 if i in eos_indices_chosen else p for i, p in enumerate(chosen_tokens["attention_mask"])
            ]
            chosen_tokens["attention_mask"] = new_attention_mask_c

            eos_indices_rejected = [i for i, x in enumerate(rejected_tokens["input_ids"]) if x == eos_token_id]
            new_attention_mask_r = [
                0 if i in eos_indices_rejected else p for i, p in enumerate(rejected_tokens["attention_mask"])
            ]
            rejected_tokens["attention_mask"] = new_attention_mask_r

            # add EOS token to end of prompt
            chosen_tokens["input_ids"].append(self.tokenizer.eos_token_id)
            chosen_tokens["attention_mask"].append(1)

            rejected_tokens["input_ids"].append(self.tokenizer.eos_token_id)
            rejected_tokens["attention_mask"].append(1)

            longer_response_length = max(len(chosen_tokens["input_ids"]), len(rejected_tokens["input_ids"]))
            # print("*** longer_response_length ", longer_response_length, "is of type ", type(longer_response_length))
            # print("*** self.max_length ", self.max_length, "is of type ", type(self.max_length))
            # print("*** self.min_prefix_length ", self.min_prefix_length, "is of type ", type(self.min_prefix_length))
            max_prefix_length = max(self.min_prefix_length, self.max_length - longer_response_length)
            max_suffix_len = self.max_length - max_prefix_length
            
            # if combined sequence is too long, truncate the prompt
            if len(prompt_tokens["input_ids"]) + longer_response_length > self.max_length:
                if self.truncation_mode == "keep_start":
                    prompt_tokens = {k: v[: max_prefix_length] for k, v in prompt_tokens.items()}
                elif self.truncation_mode == "keep_end":
                    prompt_tokens = {k: v[-max_prefix_length :] for k, v in prompt_tokens.items()}
                else:
                    raise ValueError(f"Unknown truncation mode: {self.truncation_mode}")

            # if that's still too long, truncate the response
            if len(prompt_tokens["input_ids"]) + longer_response_length > self.max_length:
                chosen_tokens = {k: v[: max_suffix_len] for k, v in chosen_tokens.items()}
                rejected_tokens = {
                    k: v[: max_suffix_len] for k, v in rejected_tokens.items()
                }

            # Create labels
            chosen_sequence_tokens = {k: prompt_tokens[k] + chosen_tokens[k] for k in chosen_tokens}
            rejected_sequence_tokens = {k: prompt_tokens[k] + rejected_tokens[k] for k in rejected_tokens}
            chosen_sequence_tokens["labels"] = chosen_sequence_tokens["input_ids"][:]
            chosen_sequence_tokens["labels"][: len(prompt_tokens["input_ids"])] = [self.label_pad_token_id] * len(
                prompt_tokens["input_ids"]
            )
            rejected_sequence_tokens["labels"] = rejected_sequence_tokens["input_ids"][:]
            rejected_sequence_tokens["labels"][: len(prompt_tokens["input_ids"])] = [self.label_pad_token_id] * len(
                prompt_tokens["input_ids"]
            )

            for k, toks in {
                "chosen": chosen_sequence_tokens,
                "rejected": rejected_sequence_tokens,
                "prompt": prompt_tokens,
            }.items():
                for type_key, tokens in toks.items():
                    if type_key == "token_type_ids":
                        continue
                    batch[f"{k}_{type_key}"] = tokens

        else:
            max_suffix_length = self.max_length - self.min_prefix_length
            chosen_tokens = self.tokenizer(
                chosen, truncation=True, max_length=max_suffix_length, add_special_tokens=True
            )
            rejected_tokens = self.tokenizer(
                rejected, truncation=True, max_length=max_suffix_length, add_special_tokens=True
            )
            max_prefix_length = self.max_length - max(len(chosen_tokens["input_ids"]), len(rejected_tokens["input_ids"]))
            prompt_tokens = self.tokenizer(
                prompt, truncation=True, max_length=max_prefix_length, add_special_tokens=True
            )

            batch["chosen_labels"] = chosen_tokens["input_ids"]
            batch["rejected_labels"] = rejected_tokens["input_ids"]
            batch["prompt_input_ids"] = prompt_tokens["input_ids"]
            batch["prompt_attention_mask"] = prompt_tokens["attention_mask"]

            if self.model is not None and hasattr(self.model, "prepare_decoder_input_ids_from_labels"):
                batch["rejected_decoder_input_ids"] = self.model.prepare_decoder_input_ids_from_labels(
                    labels=batch["rejected_labels"]
                )
                batch["chosen_decoder_input_ids"] = self.model.prepare_decoder_input_ids_from_labels(
                    labels=batch["chosen_labels"]
                )

        batch["prompt"] = prompt
        batch["chosen"] = prompt + chosen
        batch["rejected"] = prompt + rejected
        batch["chosen_response_only"] = chosen
        batch["rejected_response_only"] = rejected

        return batch

    def collate(self, batch):
        # first, pad everything to the same length
        padded_batch = {}
        for k in batch[0].keys():
            if k.endswith("_input_ids") or k.endswith("_attention_mask") or k.endswith("_labels"):
                if self.is_encoder_decoder:
                    to_pad = [torch.LongTensor(ex[k]) for ex in batch]

                    if (k.startswith("prompt")) and (k.endswith("input_ids")):
                        padding_value = self.tokenizer.pad_token_id
                    elif k.endswith("_attention_mask"):
                        padding_value = 0
                    elif (k.startswith("chosen")) or (k.startswith("rejected")) or ("decoder" in k):
                        padding_value = self.label_pad_token_id
                    else:
                        raise ValueError(f"Unexpected key in batch '{k}'")
                    padded_batch[k] = pad_sequence(to_pad, batch_first=True, padding_value=padding_value)
                else:
                    # adapted from https://stackoverflow.com/questions/73256206
                    if "prompt" in k:
                        to_pad = [torch.LongTensor(ex[k][::-1]) for ex in batch]
                    else:
                        to_pad = [torch.LongTensor(ex[k]) for ex in batch]
                    if k.endswith("_input_ids"):
                        padding_value = self.tokenizer.pad_token_id
                    elif k.endswith("_labels"):
                        padding_value = self.label_pad_token_id
                    elif k.endswith("_attention_mask"):
                        padding_value = self.padding_value
                    else:
                        raise ValueError(f"Unexpected key in batch '{k}'")

                    padded_batch[k] = pad_sequence(to_pad, batch_first=True, padding_value=padding_value)
                    # for the prompt, flip back so padding is on left side
                    if "prompt" in k:
                        padded_batch[k] = padded_batch[k].flip(dims=[1])
            else:
                padded_batch[k] = [ex[k] for ex in batch]

        return padded_batch
    
    def process_one(self, example: tuple[str | list[str], list[str]] | DatasetEntryRm) -> tuple[str, list[str]]:
        if isinstance(self.tokenizer, (LlamaTokenizer, LlamaTokenizerFast)):
            prefix_end_token = "\n\n"
            reply_end_token = ""
        else:
            prefix_end_token = self.tokenizer.eos_token
            reply_end_token = self.tokenizer.eos_token

        if isinstance(example, DatasetEntryRm):
            prefix, replies = example.get_formatted(
                prefix_end_token=prefix_end_token,
                reply_end_token=reply_end_token,
                use_system_tag=self.use_system_tag,
                system_property_dropout=self.system_property_dropout,
                system_add_length=self.system_add_length,
                max_replies=self.max_replies,
            )
        else:
            messages, replies = example
            if self.max_replies:
                assert self.max_replies > 1, "max_replies parameter must be > 1 or None"
                if len(replies) > self.max_replies:
                    replies = replies[: self.max_replies]

            if messages is None or len(messages) == 1 and messages[0] is None:
                # special handling for non-dialogue datasets like Hellaswag
                prefix = ""
                replies = [r + reply_end_token for r in replies]
            else:
                # append eos token to each messages
                prefix = "".join(format_pairs(messages, eos_token=prefix_end_token))
                replies = [format_reply(r, eos_token=reply_end_token) for r in replies]
                
        return prefix, replies
    
    # def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:
    #     tokenized_batch = []

    #     for feature in features:
    #         prompt = feature["prompt"]
    #         chosen = feature["chosen"]
    #         rejected = feature["rejected"]

    #         batch_element = self.tokenize_batch_element(prompt, chosen, rejected)
    #         tokenized_batch.append(batch_element)

    #     # return collated batch
    #     return self.collate(tokenized_batch)
    def __call__(self, examples: list[tuple[str | list[str] | None, list[str]]] | list[DatasetEntryRm]
    ) -> Dict[str, Any]:
        
        features = []
        for example in examples:
            prefix, replies = self.process_one(example)
            for chosen, rejected in zip(replies[:-1], replies[1:]):
                # n replies result in n-1 comparisons
                features.append({"prompt": prefix, "chosen": chosen, "rejected": rejected})
        
        tokenized_batch = []
        for feature in features:
            prompt = feature["prompt"]
            chosen = feature["chosen"]
            rejected = feature["rejected"]

            batch_element = self.tokenize_batch_element(prompt, chosen, rejected)
            tokenized_batch.append(batch_element)

        # return collated batch
        return self.collate(tokenized_batch)
    

def argument_parsing(notebook: bool = False, notebook_args: Sequence[str] | None = None):
    parser = argparse.ArgumentParser()
    parser.add_argument("--configs", nargs="+", required=True)
    parser.add_argument("--local_rank", type=int, default=-1)
    parser.add_argument("--deepspeed", action="store_true")
    parser.add_argument("--no-deepspeed", dest="deepspeed", action="store_false")
    parser.add_argument("--wandb-entity", type=str, default="open-assistant")
    parser.add_argument("--resume_from_checkpoint", action="store_true", help="Resume from last saved checkpoint")
    parser.add_argument("--rng_seed", type=int, help="rng seed")
    parser.add_argument("--show_dataset_stats", action="store_true", help="Show dataset stats", default=False)
    parser.set_defaults(deepspeed=False)

    if notebook:
        args, remaining = parser.parse_known_args(notebook_args)
    else:
        args, remaining = parser.parse_known_args()

    # Config from YAML
    conf = {}
    configs = read_yamls("./configs")
    for name in args.configs:
        if "," in name:
            for n in name.split(","):
                conf.update(configs[n])
        else:
            conf.update(configs[name])

    conf["wandb_entity"] = args.wandb_entity
    conf["local_rank"] = args.local_rank
    conf["deepspeed"] = args.deepspeed
    conf["resume_from_checkpoint"] = args.resume_from_checkpoint
    if args.rng_seed is not None:
        conf["rng_seed"] = args.rng_seed
    conf["show_dataset_stats"] = args.show_dataset_stats

    # get the world size in deepspeed
    if conf["deepspeed"]:
        conf["world_size"] = int(os.getenv("WORLD_SIZE", default="1"))
    else:
        conf["world_size"] = 1

    # Override config from command-line
    parser = argparse.ArgumentParser()
    for key, value in conf.items():
        type_ = type(value) if value is not None else str
        if type_ == bool:
            type_ = _strtobool
        parser.add_argument(f"--{key}", type=type_, default=value)

    return parser.parse_args(remaining)


def main():
    training_conf = argument_parsing()
    if not training_conf.deepspeed or training_conf.local_rank == 0:
        print(f"trainig_conf = {training_conf}")

    init_rng(training_conf)

    # load sft model
    tokenizer = get_tokenizer(training_conf)
    model = get_model(training_conf, tokenizer)
    # make model_ref a copy of model and with no gradients
    model_ref = get_model(training_conf, tokenizer)
    model_ref.eval()

    # load rm dataset
    train, evals = get_dataset_rm(training_conf, mode="rm")
    
    # label_pad_token_id: int = -100
    # padding_value: int = 0
    data_collator = CustomDPODataCollatorWithPadding(
        tokenizer,
        model=model,
        max_length=training_conf.max_length,
        max_replies=training_conf.max_replies,
        use_system_tag=training_conf.use_system_tag,
        system_property_dropout=training_conf.system_property_dropout,
        system_add_length=training_conf.system_add_length,
    )

    show_dataset_stats = (training_conf.verbose or training_conf.show_dataset_stats) and (
        not training_conf.deepspeed or training_conf.local_rank == 0
    )
    if show_dataset_stats:
        print("Dataset stats before sampling:")
        total = len(train)
        for d in train.datasets:
            if isinstance(d, Subset):
                name = f"Subset of {type(d.dataset).__name__}"
                if hasattr(d.dataset, "name"):
                    name += f" ({d.dataset.name})"
            else:
                name = type(d).__name__
                if hasattr(d, "name"):
                    name += f" ({d.name})"
            print(f"{name}: {len(d)} ({len(d) / total:%})")
        print(f"Total train: {total}")

    # TODO: restore sampler
    # if training_conf.use_custom_sampler:
    #     samples_length = None
    #     if training_conf.sort_by_length:
    #         samples_length = list(
    #             map(
    #                 lambda x: train_collate_fn.process_one(x, return_length=True),
    #                 tqdm(train, desc="Calculating lengths per sample"),
    #             )
    #         )
    #     sampler = PerDatasetSampler.build_sampler_from_config(
    #         training_conf,
    #         train.datasets,
    #         rank=training_conf.local_rank,
    #         world_size=training_conf.world_size,
    #         samples_length=samples_length,
    #         verbose=show_dataset_stats,
    #     )
    # else:
    #     sampler = None

    optimizer = OptimizerNames.ADAMW_BNB if training_conf.quantization else OptimizerNames.ADAMW_HF

    if training_conf.quantization:
        import bitsandbytes

        for module in model.modules():
            if isinstance(module, torch.nn.Embedding):
                bitsandbytes.optim.GlobalOptimManager.get_instance().register_module_override(
                    module, "weight", {"optim_bits": 32}
                )

    if training_conf.fuse_gelu:
        model = fuse_gelu(model)

    output_dir = (
        training_conf.output_dir
        if training_conf.output_dir
        else f"{training_conf.model_name}-{training_conf.log_dir}-finetuned"
    )

    args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=training_conf.num_train_epochs,
        warmup_steps=training_conf.warmup_steps,
        learning_rate=float(training_conf.learning_rate),
        deepspeed=training_conf.deepspeed_config if training_conf.deepspeed else None,
        optim=optimizer,
        fp16=training_conf.dtype in ["fp16", "float16"],
        bf16=training_conf.dtype in ["bf16", "bfloat16"],
        local_rank=training_conf.local_rank,
        gradient_checkpointing=training_conf.gradient_checkpointing,
        gradient_accumulation_steps=training_conf.gradient_accumulation_steps,
        per_device_train_batch_size=training_conf.per_device_train_batch_size,
        per_device_eval_batch_size=training_conf.per_device_eval_batch_size,
        adam_beta1=training_conf.adam_beta1,
        adam_beta2=training_conf.adam_beta2,
        adam_epsilon=float(training_conf.adam_epsilon),
        weight_decay=training_conf.weight_decay,
        max_grad_norm=training_conf.max_grad_norm,
        logging_steps=training_conf.logging_steps,
        save_total_limit=training_conf.save_total_limit,
        evaluation_strategy="steps",
        eval_steps=training_conf.eval_steps,
        save_strategy=training_conf.save_strategy,
        save_steps=training_conf.save_steps,
        eval_accumulation_steps=training_conf.eval_accumulation_steps,
        resume_from_checkpoint=training_conf.resume_from_checkpoint,
        report_to="wandb" if training_conf.log_wandb else None,
    )

    if not training_conf.log_wandb:
        os.environ["WANDB_MODE"] = "offline"

    if training_conf.log_wandb and (not training_conf.deepspeed or training_conf.local_rank == 0):
        import wandb

        wandb.init(
            project="dpo",
            entity=training_conf.wandb_entity,
            resume=training_conf.resume_from_checkpoint,
            name=f"{training_conf.model_name}-{training_conf.log_dir}-dpo",
            config=training_conf,
        )
    compute_metrics = RewardMetrics(training_conf.metrics)
    
    # trainer = RMTrainer(
    #     model=model,
    #     args=args,
    #     sampler=sampler,
    #     train_collate_fn=train_collate_fn,
    #     loss_function=training_conf.loss_fn,
    #     score_l2_reg=training_conf.score_l2_reg,
    #     train_dataset=train,
    #     eval_dataset=evals,
    #     data_collator=eval_collate_fn,
    #     tokenizer=tokenizer,
    #     compute_metrics=compute_metrics,
    # )
    trainer = DPOTrainer(
        model,
        model_ref,
        args=args,
        beta=training_conf.dpo_beta,
        train_dataset=train,
        eval_dataset=evals,
        data_collator=data_collator,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
    )
    
    trainer.train(resume_from_checkpoint=training_conf.resume_from_checkpoint)
    trainer.save_model()
    tokenizer.save_pretrained(output_dir)


if __name__ == "__main__":
    main()
